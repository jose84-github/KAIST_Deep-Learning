{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6gHiH-I7uFa"
   },
   "source": [
    "#Improving Computer Vision Accuracy using Convolutions\n",
    "\n",
    "In the previous lessons you saw how to do fashion recognition using a Deep Neural Network (DNN) containing three layers -- the input layer (in the shape of the data), the output layer (in the shape of the desired output) and a hidden layer. You experimented with the impact of different sized of hidden layer, number of training epochs etc on the final accuracy.\n",
    "\n",
    "For convenience, here's the entire code again. Run it and take a note of the test accuracy that is printed out at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcsRtq9OLorS",
    "outputId": "a4b6185d-b5f5-45ff-978a-532751032d9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 621us/step - loss: 0.6514 - accuracy: 0.7719\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 627us/step - loss: 0.3913 - accuracy: 0.8624\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 623us/step - loss: 0.3376 - accuracy: 0.8756\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 628us/step - loss: 0.3225 - accuracy: 0.8823\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 625us/step - loss: 0.2918 - accuracy: 0.8941\n",
      "313/313 [==============================] - 0s 423us/step - loss: 0.3536 - accuracy: 0.8720\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images / 255.0\n",
    "test_images=test_images / 255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zldEXSsF8Noz"
   },
   "source": [
    "Your accuracy is probably about 89% on training and 87% on validation...not bad...But how do you make that even better? One way is to use something called Convolutions. I'm not going to details on Convolutions here, but the ultimate concept is that they narrow down the content of the image to focus on specific, distinct, details. \n",
    "\n",
    "If you've ever done image processing using a filter (like this: https://en.wikipedia.org/wiki/Kernel_(image_processing)) then convolutions will look very familiar.\n",
    "\n",
    "In short, you take an array (usually 3x3 or 5x5) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can do things like edge detection. So, for example, if you look at the above link, you'll see a 3x3 that is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has the edges enhanced.\n",
    "\n",
    "This is perfect for computer vision, because often it's features that can get highlighted like this that distinguish one item for another, and the amount of information needed is then much less...because you'll just train on the highlighted features.\n",
    "\n",
    "That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focussed, and possibly more accurate.\n",
    "\n",
    "Run the below code -- this is the same neural network as earlier, but this time with Convolutional layers added first. It will take longer, but look at the impact on the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0tFgT1MMKi6",
    "outputId": "eacdd0ce-3528-4199-c433-c7423006e036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.6015 - accuracy: 0.7817\n",
      "Epoch 2/5\n",
      " 430/1875 [=====>........................] - ETA: 21s - loss: 0.2986 - accuracy: 0.8931"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a07706d4a64d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss = model.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRLfZ0jt-fQI"
   },
   "source": [
    "It's likely gone up to about 93% on the training data and 91% on the validation data. \n",
    "\n",
    "That's significant, and a step in the right direction!\n",
    "\n",
    "Try running it for more epochs -- say about 20, and explore the results! But while the results might seem really good, the validation results may actually go down, due to something called 'overfitting' which will be discussed later. \n",
    "\n",
    "(In a nutshell, 'overfitting' occurs when the network learns the data from the training set really well, but it's too specialised to only that data, and as a result is less effective at seeing *other* data. For example, if all your life you only saw red shoes, then when you see a red shoe you would be very good at identifying it, but blue suade shoes might confuse you...and you know you should never mess with my blue suede shoes.)\n",
    "\n",
    "Then, look at the code again, and see, step by step how the Convolutions were built:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaLX5cgI_JDb"
   },
   "source": [
    "Step 1 is to gather the data. You'll notice that there's a bit of a change here in that the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape. \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SS_W_INc_kJQ"
   },
   "source": [
    "Next is to define your model. Now instead of the input layer at the top, you're going to add a Convolution. The parameters are:\n",
    "\n",
    "1. The number of convolutions you want to generate. Purely arbitrary, but good to start with something in the order of 32\n",
    "2. The size of the Convolution, in this case a 3x3 grid\n",
    "3. The activation function to use -- in this case we'll use relu, which you might recall is the equivalent of returning x when x>0, else returning 0\n",
    "4. In the first layer, the shape of the input data.\n",
    "\n",
    "You'll follow the Convolution with a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convlution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. Without going into too much detail here, the idea is that it creates a 2x2 array of pixels, and picks the biggest one, thus turning 4 pixels into 1. It repeats this across the image, and in so doing halves the number of horizontal, and halves the number of vertical pixels, effectively reducing the image by 25%.\n",
    "\n",
    "You can call model.summary() to see the size and shape of the network, and you'll notice that after every MaxPooling layer, the image size is reduced in this way. \n",
    "\n",
    "\n",
    "```\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMorM6daADjA"
   },
   "source": [
    "Add another convolution\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1-x-kZF4_tC"
   },
   "source": [
    "Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version\n",
    "\n",
    "```\n",
    "  tf.keras.layers.Flatten(),\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPtqR23uASjX"
   },
   "source": [
    "The same 128 dense layers, and 10 output layers as in the pre-convolution example:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0GSsjUhAaSj"
   },
   "source": [
    "Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the test set.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXx_LX3SAlFs"
   },
   "source": [
    "# Visualizing the Convolutions and Pooling\n",
    "\n",
    "This code will show us the convolutions graphically. The print (test_labels[;100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-6nX4QsOku6",
    "outputId": "36aa0acb-55c0-4f45-a804-fb1f7ce6f65f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
      " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
      " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "9FGsHhv6JvDx",
    "outputId": "01afbfcc-7d89-4546-e786-9f02c70bb950"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f, axarr = plt.subplots(3,4)\n",
    "FIRST_IMAGE=0\n",
    "SECOND_IMAGE=7\n",
    "THIRD_IMAGE=26\n",
    "CONVOLUTION_NUMBER = 1\n",
    "from tensorflow.keras import models\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "for x in range(0,4):\n",
    "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[0,x].grid(False)\n",
    "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[1,x].grid(False)\n",
    "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[2,x].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KVPZqgHo5Ux"
   },
   "source": [
    "EXERCISES\n",
    "\n",
    "1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.\n",
    "\n",
    "2. Remove the final Convolution. What impact will this have on accuracy or training time?\n",
    "\n",
    "3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.\n",
    "\n",
    "4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it. \n",
    "\n",
    "5. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZpYRidBXpBPM",
    "outputId": "05143595-0b17-4dcb-bfd1-589e16e4caf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2908 - accuracy: 0.9118\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0555 - accuracy: 0.9838\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0325 - accuracy: 0.9897\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0189 - accuracy: 0.9938\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0136 - accuracy: 0.9958\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0089 - accuracy: 0.9969\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0062 - accuracy: 0.9982\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0050 - accuracy: 0.9983\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0050 - accuracy: 0.9985\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0025 - accuracy: 0.9994\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0439 - accuracy: 0.9882\n",
      "0.9882000088691711\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CuS_0r3rmMVF",
    "outputId": "c4bbffad-c94a-44f2-bf2b-fd0060474566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_449 (Conv2D)          (None, 24, 24, 4)         104       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_435 (MaxPoolin (None, 12, 12, 4)         0         \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 12, 12, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_450 (Conv2D)          (None, 8, 8, 12)          1212      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_436 (MaxPoolin (None, 4, 4, 12)          0         \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 4, 4, 12)          0         \n",
      "_________________________________________________________________\n",
      "flatten_57 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 16)                3088      \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 4,574\n",
      "Trainable params: 4,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.0407 - accuracy: 0.6167\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5450 - accuracy: 0.7960\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4757 - accuracy: 0.8229\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4364 - accuracy: 0.8386\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4243 - accuracy: 0.8428\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4077 - accuracy: 0.8472\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3917 - accuracy: 0.8537\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3718 - accuracy: 0.8594\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3682 - accuracy: 0.8629\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3646 - accuracy: 0.8640\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_451 (Conv2D)          (None, 24, 24, 4)         104       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_437 (MaxPoolin (None, 12, 12, 4)         0         \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 12, 12, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_452 (Conv2D)          (None, 8, 8, 12)          1212      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_438 (MaxPoolin (None, 4, 4, 12)          0         \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 4, 4, 12)          0         \n",
      "_________________________________________________________________\n",
      "flatten_58 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 16)                3088      \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 4,574\n",
      "Trainable params: 4,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1180/1875 [=================>............] - ETA: 1s - loss: 1.2242 - accuracy: 0.5592"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-0f22558f7f0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mbest_paramters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m           **self.filter_sk_params(self.build_fn.__call__))\n\u001b[1;32m    156\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_sk_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     if (losses.is_categorical_crossentropy(self.model.loss) and\n",
      "\u001b[0;32m<ipython-input-51-0f22558f7f0f>\u001b[0m in \u001b[0;36mbuild_LeNet\u001b[0;34m(dropout_rate, epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0;31m#test_loss, test_acc = model.evaluate(test_images, test_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m#print(\"Test Accuracy is {}\".format(test_acc))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LeNet 1 만들기 - 2 Conv Layer + GridSearch\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "\n",
    "def build_LeNet(dropout_rate, epochs):\n",
    "  model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(4, (5,5), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(rate = dropout_rate),\n",
    "\n",
    "    tf.keras.layers.Conv2D(12, (5,5), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(rate = dropout_rate),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate = dropout_rate),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "  model.summary()\n",
    "  model.fit(training_images, training_labels, epochs=epochs)\n",
    "  #test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "  #print(\"Test Accuracy is {}\".format(test_acc))\n",
    "\n",
    "  return model\n",
    "\n",
    "model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn = build_LeNet, verbose=0)\n",
    "\n",
    "param_grid = {\n",
    "    'batch_size':[10,20],\n",
    "    'epochs':[10, 30],\n",
    "    'dropout_rate':[0.05,0.1]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator= model, param_grid=param_grid, cv=2)\n",
    "grid = grid.fit(test_images, test_labels)\n",
    "\n",
    "best_paramters = grid.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMLwcuf4FamH"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Test Accuracy is {}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4-Using-Convolutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
